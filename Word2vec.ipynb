{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3pOoAxVP1V4SUqLn2oc9D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SnkhchyanV/NLP/blob/main/Word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ME9ChC_JKakW",
        "outputId": "092de54b-fa36-4cf9-98e3-ebc0ed6b3387",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import zipfile\n",
        "\n",
        "import gensim\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import random\n",
        "from collections import deque\n",
        "from itertools import chain\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/DataSets/corpus_100k.zip'\n",
        "zip_ref = zipfile.ZipFile(data_path , 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall('/content/sample_data/corpus_100k') #Extracts the files into the /corpus_100k folder\n",
        "zip_ref.close()\n"
      ],
      "metadata": {
        "id": "QUwMG9GwIF-X"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/sample_data/corpus_100k/corpus_100k', 'r', encoding='utf-8', errors='replace') as f:\n",
        "#   sentences = [s.strip() for s in f.readlines()]\n",
        "\n",
        "# print(f'Number of sentences: {len(sentences)}')\n",
        "# use_first_n = 300000\n",
        "# sentences = sentences[:use_first_n]\n",
        "# print(f'Using: {len(sentences)}')"
      ],
      "metadata": {
        "id": "ZcNIWZIzK6DD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def data_preprocessing(data):\n",
        "#   translation_table = str.maketrans('', '', \"«»()+-=-,՝.․։՜՛֊՟՚\")\n",
        "#   sentences = [(sentence).lower().translate(translation_table) for sentence in data]\n",
        "#   return sentences\n",
        "\n",
        "# data = data_preprocessing(sentences)\n",
        "\n",
        "# def create_vocabulary(sentences):\n",
        "#   i = 0\n",
        "#   word2idx ={}\n",
        "#   idx2word = {}\n",
        "#   unique = set()\n",
        "#   freqs = {}\n",
        "\n",
        "#   for sentence in sentences:\n",
        "#     for word in sentence.split(' '):\n",
        "#       if (word not in unique) and (len(word.strip()) != 0):\n",
        "#         word2idx[word] = i\n",
        "#         idx2word[i] = word\n",
        "#         i += 1\n",
        "#         unique.add(word)\n",
        "#         freqs[word] = 1\n",
        "#       elif word in unique:\n",
        "#         freqs[word] += 1\n",
        "#   return unique, word2idx, idx2word, freqs\n",
        "\n",
        "# vocab, word2idx, idx2word, freqs = create_vocabulary(data)\n",
        "\n",
        "# # Altering the distribution to perform negative sampling\n",
        "# totalWords = sum([freq**(3/4) for freq in freqs.values()])\n",
        "# wordProb = {word:(freq)**(3/4)/totalWords for word, freq in freqs.items()}"
      ],
      "metadata": {
        "id": "n5-4LCTvPtgc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/sample_data/corpus_100k/corpus_100k', 'r', encoding='utf-8', errors='replace') as f:\n",
        "    sentences = [s.strip() for s in f.readlines()]\n",
        "\n",
        "use_first_n = 300000\n",
        "sentences = sentences[:use_first_n]\n",
        "\n",
        "def data_preprocessing(data):\n",
        "    translation_table = str.maketrans('', '', \"«»()+-=-,՝.․։՜՛֊՟՚\")\n",
        "    sentences = [sentence.lower().translate(translation_table) for sentence in data]\n",
        "    return sentences\n",
        "\n",
        "data = data_preprocessing(sentences)\n",
        "\n",
        "def create_vocabulary(sentences):\n",
        "    i = 0\n",
        "    word2idx = {}\n",
        "    idx2word = {}\n",
        "    unique = set()\n",
        "    freqs = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            if (word not in unique) and (len(word.strip()) != 0):\n",
        "                word2idx[word] = i\n",
        "                idx2word[i] = word\n",
        "                i += 1\n",
        "                unique.add(word)\n",
        "                freqs[word] = 1\n",
        "            elif word in unique:\n",
        "                freqs[word] += 1\n",
        "    return unique, word2idx, idx2word, freqs\n",
        "\n",
        "vocab, word2idx, idx2word, freqs = create_vocabulary(data)\n",
        "totalWords = sum([freq ** (3/4) for freq in freqs.values()])\n",
        "wordProb = {word: (freq) ** (3/4) / totalWords for word, freq in freqs.items()}\n"
      ],
      "metadata": {
        "id": "JvpPyRkQ3ukw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def skipgram_data_generator(sentences, window_size, batch_size, vocab):\n",
        "    for sentence in sentences:\n",
        "        padded_sentence = [None] * (window_size) + sentence + [None] * (window_size)\n",
        "        window = deque(maxlen=2 * window_size + 1)\n",
        "        for word in padded_sentence:\n",
        "            if len(window) == 2 * window_size + 1:\n",
        "                target_word = window[window_size]\n",
        "                context_words = list(chain(list(window)[:window_size], list(window)[window_size + 1:]))\n",
        "                for context_word in context_words:\n",
        "                    if target_word in vocab and context_word in vocab:\n",
        "                        yield target_word, context_word\n",
        "            window.append(word)\n",
        "\n",
        "        if None in window:\n",
        "            yield None, None\n",
        "\n",
        "\n",
        "data = [sentence.split() for sentence in data ]\n",
        "\n",
        "window_size = 2\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "batches = []\n",
        "current_batch = []\n",
        "\n",
        "for target, context in skipgram_data_generator(data[:10], window_size, batch_size, vocab):\n",
        "    if target is not None and context is not None:\n",
        "        current_batch.append((target, context))\n",
        "    if len(current_batch) == batch_size:\n",
        "        batches.append(current_batch)\n",
        "        current_batch = []\n",
        "\n",
        "if current_batch:\n",
        "    batches.append(current_batch)\n",
        "\n",
        "\n",
        "values = list(wordProb.keys())\n",
        "probabilities = list(wordProb.values())\n",
        "neg_samples = []\n",
        "num_neg_samples = 10\n",
        "targets = []\n",
        "contexts = []\n",
        "labels = []\n",
        "\n",
        "for i, batch in enumerate(batches):\n",
        "    for target, context in batch:\n",
        "      context_arr = []\n",
        "      targets.append(word2idx[target])\n",
        "      context_arr.append(word2idx[context])\n",
        "      i = 0\n",
        "      while(i < num_neg_samples):\n",
        "          word = random.choices(values, weights=probabilities, k=1)[0]\n",
        "          if word != target and word != context:\n",
        "            context_arr.append(word2idx[word])\n",
        "            i += 1\n",
        "\n",
        "      label_arr = np.zeros(len(context_arr), dtype=int)\n",
        "      label_arr[0] = 1\n",
        "      contexts.append(context_arr)\n",
        "      labels.append(list(label_arr))\n"
      ],
      "metadata": {
        "id": "qFXUnvk3ylU5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2Vec(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim=200):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_weights = tf.Variable(tf.random.uniform(\n",
        "            shape=(self.vocab_size, embedding_dim),\n",
        "            minval=-0.5 / embedding_dim,\n",
        "            maxval=0.5 / embedding_dim,\n",
        "        ))\n",
        "        self.output_weights = tf.Variable(tf.random.uniform(\n",
        "            shape=(embedding_dim, self.vocab_size),\n",
        "            minval=-0.5 / embedding_dim,\n",
        "            maxval=0.5 / embedding_dim,\n",
        "        ))\n",
        "\n",
        "    def call(self, data):\n",
        "        hidden_layer = tf.transpose(tf.nn.embedding_lookup(self.hidden_weights, data))\n",
        "        output_init = tf.linalg.matmul(hidden_layer, self.output_weights)\n",
        "\n",
        "        y = tf.nn.softmax(output_init)\n",
        "\n",
        "        return y, hidden_layer, output_init\n",
        "\n",
        "    def training(self, targ, cont, lab, num_iter=10000):\n",
        "        for _ in range(num_iter):\n",
        "            for i, word_idx in enumerate(targ):\n",
        "                cont_pos = cont[i][0]\n",
        "                context_indices_neg = cont[i][1:]\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    pos_logits = tf.reduce_sum(tf.multiply(tf.nn.embedding_lookup(self.output_weights, cont_pos), self.hidden_weights[word_idx]))\n",
        "                    neg_logits = tf.linalg.matmul(tf.nn.embedding_lookup(self.output_weights, context_indices_neg), tf.transpose(tf.expand_dims(self.hidden_weights[word_idx], axis=1)))\n",
        "\n",
        "                    loss = tf.reduce_mean(-tf.math.log(tf.nn.sigmoid(pos_logits)) - tf.math.log(tf.nn.sigmoid(-neg_logits)))\n",
        "\n",
        "                gradients = tape.gradient(loss, self.trainable_variables)\n",
        "                optimizer = tf.optimizers.Adam(learning_rate=1)\n",
        "                optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "                print(loss)\n",
        "\n",
        "\n",
        "\n",
        "word2vec = Word2Vec(len(vocab))\n",
        "word2vec.training(targets, contexts, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "K8o5qmGmyoKK",
        "outputId": "51b3888e-4fa2-4f69-a5c0-efb3fbebfa5f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2f96a492d69e>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-2f96a492d69e>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self, targ, cont, lab, num_iter)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     \u001b[0mpos_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                     \u001b[0mneg_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_indices_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7261\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7262\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [166229] vs. [200] [Op:Mul]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r1FMDWs40C0m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}